# spark jobs ( subscribe to kafka which is the middle to move data between data sources and consumrs) 

# kafka is good for low-latency data pipelines 

# the spark job will be batch ingestion 
# process the data and then store processed data in cloud storage or send it to downstream systems
# like dashboard) 




## use spark to transform and flatten the json into table or sql format so that it can be pushed onto the cloud so that the dashboard can be created